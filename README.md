# Eagle Ford ML Pipeline

Complete end-to-end machine learning pipeline for Eagle Ford Formation gamma ray log prediction from raw LAS files to trained models.

## ğŸ—ï¸ Pipeline Architecture

This pipeline consists of 4 interconnected scripts that form a complete ML workflow:

### 1. **Preprocessing Pipeline** (`src/preprocessing_pipeline.py`)
**Class:** `EagleFordPreprocessor`  
**Purpose:** Raw LAS file processing and data standardization
- Reads all `.las` files from raw folder using `lasio`
- Extracts curves into unified DataFrame with `API_NUMBER` tagging
- Handles missing values (`-999.25` â†’ `NaN`) with KNN imputation
- Applies quality control filters and depth resampling
- Implements Box-Cox normalization and outlier detection
- **Output:** `master_dataset.csv` with 98.6% data completeness

### 2. **Feature Engineering** (`src/feature_engineering.py`)
**Class:** `EagleFordFeatureEngineer`  
**Purpose:** Advanced feature creation and engineering
- Creates 1,130+ engineered features from 22 base curves (20x expansion)
- Rolling window statistics (5, 10, 20 point windows)
- Gradient and geological indicator features
- Cross-curve correlation features
- Well-based train/test splitting to prevent data leakage
- **Output:** Feature matrices with comprehensive geological context

### 3. **ML Models** (`src/ml_models.py`)
**Class:** `EagleFordMLTrainer`  
**Purpose:** Model training with hyperparameter optimization
- Random Forest and XGBoost baseline models
- RandomizedSearchCV + GridSearchCV hyperparameter tuning
- SHAP interpretability analysis
- Cross-validation with geological awareness
- Model persistence and evaluation metrics
- **Output:** Trained models with comprehensive performance reports

### 4. **Pipeline Orchestrator** (`eagle_ford_pipeline.py`) â­
**Class:** `EagleFordPipeline`  
**Purpose:** Complete end-to-end execution with platform compatibility
- Connects all 3 pipeline stages seamlessly
- Enhanced platform detection (Kaggle, Colab, Mac M2, VSCode)
- Automatic dependency management and error handling
- Configuration management with platform-specific optimizations
- Comprehensive logging and state management

## ğŸ”§ Configuration System

The pipeline uses a sophisticated configuration system (`EagleFordConfig`) that automatically optimizes for your platform:

### Platform-Specific Optimizations

**Kaggle Notebooks:**
```python
n_jobs: -1                    # Use all cores
max_memory_usage: 0.9         # Aggressive memory usage
cv_folds: 5                   # Full cross-validation
n_iter_search: 50             # Extended hyperparameter search
```

**Google Colab:**
```python
n_jobs: -1                    # Use all cores  
max_memory_usage: 0.85        # Conservative memory usage
chunk_processing: True        # Memory-efficient processing
```

**Mac M2 Air (8GB RAM):**
```python
n_jobs: 8                     # Optimized for M2
max_memory_usage: 0.7         # Conservative for 8GB
chunk_processing: True        # Enable chunking
normalization: None           # Skip Box-Cox for speed
cv_folds: 3                   # Reduced CV folds
```

### Core Configuration

```python
GLOBAL = {
    'target_column': 'GR',           # Gamma Ray prediction
    'random_state': 42,              # Reproducibility
    'formation': 'eagle_ford',       # Formation context
}

PROCESSING = {
    'target_step_size': 1.0,         # 1ft depth resolution
    'gr_constraints': {'min': 10.0, 'max': 250.0},  # API units
    'rolling_windows': [5, 10, 20],  # Feature windows
    'sequence_length': 50,           # Sequence modeling
}
```

## ğŸš€ Quick Start

### Setup

```bash
# Clone and navigate
cd /path/to/eagle_ford_project/code

# Install dependencies
python -m pip install --upgrade pip
pip install -r requirements.txt
```

### Run Complete Pipeline

```bash
# Auto-detect platform and run everything
python eagle_ford_pipeline.py

# Custom input/output directories
python eagle_ford_pipeline.py \
  --input-dir "../dataset/raw" \
  --output-dir "../eagle_ford_output"

# Platform-specific examples
# Kaggle/Colab
python eagle_ford_pipeline.py \
  --input-dir "/kaggle/input" \
  --output-dir "/kaggle/working"

# Mac M2 test mode (reduced processing)
python eagle_ford_pipeline.py \
  --mode test \
  --input-dir "./dataset/raw"
```

### Run Individual Stages

```bash
# Stage 1: Preprocessing only
python eagle_ford_pipeline.py --stage preprocessing

# Stage 2: Feature engineering only
python eagle_ford_pipeline.py --stage features

# Stage 3: Model training only  
python eagle_ford_pipeline.py --stage models

# Force rerun all stages
python eagle_ford_pipeline.py --force-rerun
```

### Legacy Individual Scripts

```bash
# 1. Preprocessing (legacy method)
python -m src.preprocessing_pipeline

# 2. Feature Engineering (legacy method)
python -m src.feature_engineering

# 3. ML Training (legacy method)
python -m src.ml_models
```

## ğŸ“Š Pipeline Flow

```
Raw LAS Files (31 wells)
         â†“
   ğŸ”§ Preprocessing
    (EagleFordPreprocessor)
         â†“
  master_dataset.csv (459K+ records)
         â†“
   ğŸ›ï¸ Feature Engineering
   (EagleFordFeatureEngineer)
         â†“
  Feature Matrices (1,130+ features)
         â†“
   ğŸ¤– Model Training
   (EagleFordMLTrainer)
         â†“
  Trained Models + Reports
```

## ğŸ“ Output Structure

```
eagle_ford_output/
â”œâ”€â”€ preprocessing/
â”‚   â”œâ”€â”€ master_dataset.csv           # Cleaned dataset
â”‚   â”œâ”€â”€ processing_report.json       # QC metrics
â”‚   â””â”€â”€ well_statistics.csv          # Per-well stats
â”œâ”€â”€ features/
â”‚   â”œâ”€â”€ train_features.csv           # Training features
â”‚   â”œâ”€â”€ test_features.csv            # Test features
â”‚   â”œâ”€â”€ feature_importance.csv       # Feature rankings
â”‚   â””â”€â”€ feature_engineering_report.json
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ random_forest_model.joblib   # Trained RF model
â”‚   â”œâ”€â”€ xgboost_model.joblib         # Trained XGB model
â”‚   â”œâ”€â”€ model_evaluation.json        # Performance metrics
â”‚   â”œâ”€â”€ shap_analysis/               # Interpretability
â”‚   â””â”€â”€ hyperparameter_results.json
â””â”€â”€ logs/
    â””â”€â”€ eagle_ford_pipeline_*.log    # Execution logs
```

## ğŸ› ï¸ Platform Compatibility

âœ… **Kaggle Notebooks** - Full GPU support, high-performance computing  
âœ… **Google Colab** - GPU/TPU support, cloud storage integration  
âœ… **VSCode (Mac M2)** - Optimized for M2 architecture and 8GB RAM  
âœ… **Jupyter Notebooks** - Interactive development and analysis  
âœ… **General Python** - Any Python 3.8+ environment

## ğŸ” Key Features

- **Production-Ready:** End-to-end validation with 100% file processing success
- **Research-Based:** Implements 2025 best practices for well log ML
- **Platform-Aware:** Automatic optimization for your computing environment
- **Interpretable:** Comprehensive SHAP analysis and feature importance
- **Robust:** Advanced error handling and automatic dependency management
- **Scalable:** Memory-efficient processing for datasets of any size

## ğŸ“‹ Requirements

**Minimum:**
- Python 3.8+
- 4GB RAM
- 2GB disk space

**Recommended:**
- Python 3.10+
- 8GB+ RAM
- GPU (optional, auto-detected)

## ğŸ†˜ Troubleshooting

**Import Errors:**
```bash
# Ensure you're in the code/ directory
cd code/
python eagle_ford_pipeline.py
```

**Memory Issues:**
```bash
# Use test mode for limited resources
python eagle_ford_pipeline.py --mode test
```

**Platform Detection Issues:**
- Pipeline auto-detects Kaggle, Colab, Mac M2
- Manual platform optimization available in config

The pipeline includes comprehensive error handling and will guide you through any issues. Check the generated log files for detailed troubleshooting information.
